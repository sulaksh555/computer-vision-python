{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a deep learning model using PyTorch to classify images using convolutional neural networks. The model is trained on the well-known CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html). The techniques of transfer learning (from the VGG16 model with batch normalization) and data augmentation were used to enhance the model's accuracy. A final accuracy of 87.8600 % was obtained on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the torchinfo library to obtain model summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:05.965416Z",
     "iopub.status.busy": "2021-09-04T15:12:05.96486Z",
     "iopub.status.idle": "2021-09-04T15:12:16.344304Z",
     "shell.execute_reply": "2021-09-04T15:12:16.34298Z",
     "shell.execute_reply.started": "2021-09-04T15:12:05.965313Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries and methods needed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:19.943753Z",
     "iopub.status.busy": "2021-09-04T15:12:19.943229Z",
     "iopub.status.idle": "2021-09-04T15:12:21.719589Z",
     "shell.execute_reply": "2021-09-04T15:12:21.718307Z",
     "shell.execute_reply.started": "2021-09-04T15:12:19.943718Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    Sequential,\n",
    "    Linear,\n",
    "    ReLU,\n",
    "    Dropout,\n",
    "    CrossEntropyLoss\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    RandomRotation,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomCrop,\n",
    "    ToTensor,\n",
    "    Normalize\n",
    ")\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import (\n",
    "    random_split,\n",
    "    DataLoader\n",
    ")\n",
    "from torchvision.models import vgg16_bn\n",
    "from torchinfo import summary\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out the image size, means and standard deviations expected by the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:26.627377Z",
     "iopub.status.busy": "2021-09-04T15:12:26.626998Z",
     "iopub.status.idle": "2021-09-04T15:12:26.6338Z",
     "shell.execute_reply": "2021-09-04T15:12:26.632601Z",
     "shell.execute_reply.started": "2021-09-04T15:12:26.627346Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transforms for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:30.816181Z",
     "iopub.status.busy": "2021-09-04T15:12:30.815778Z",
     "iopub.status.idle": "2021-09-04T15:12:30.824752Z",
     "shell.execute_reply": "2021-09-04T15:12:30.823258Z",
     "shell.execute_reply.started": "2021-09-04T15:12:30.816152Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    Resize(size = pretrained_size),\n",
    "    RandomRotation(degrees = 5),\n",
    "    RandomHorizontalFlip(p = 0.5),\n",
    "    RandomCrop(size = pretrained_size, \n",
    "               padding = 10),\n",
    "    ToTensor(),\n",
    "    Normalize(mean = pretrained_means,\n",
    "              std = pretrained_stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transforms for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:35.359471Z",
     "iopub.status.busy": "2021-09-04T15:12:35.359076Z",
     "iopub.status.idle": "2021-09-04T15:12:35.365364Z",
     "shell.execute_reply": "2021-09-04T15:12:35.363903Z",
     "shell.execute_reply.started": "2021-09-04T15:12:35.359442Z"
    }
   },
   "outputs": [],
   "source": [
    "test_transforms = Compose([\n",
    "    Resize(size = pretrained_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean = pretrained_means,\n",
    "              std = pretrained_stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, performing the predefined transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:39.430707Z",
     "iopub.status.busy": "2021-09-04T15:12:39.430213Z",
     "iopub.status.idle": "2021-09-04T15:12:49.758117Z",
     "shell.execute_reply": "2021-09-04T15:12:49.757059Z",
     "shell.execute_reply.started": "2021-09-04T15:12:39.430636Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = \".data\"\n",
    "\n",
    "raw_train_data = CIFAR10(root = ROOT,\n",
    "                         train = True,\n",
    "                         download = True,\n",
    "                         transform = train_transforms)\n",
    "\n",
    "test_data = CIFAR10(root = ROOT,\n",
    "                    train = False,\n",
    "                    download = True,\n",
    "                    transform = test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carve out a validation set from the raw training set. 90 % of the raw training set forms the final training set and the other 10 % forms the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:54.851103Z",
     "iopub.status.busy": "2021-09-04T15:12:54.850644Z",
     "iopub.status.idle": "2021-09-04T15:12:54.88878Z",
     "shell.execute_reply": "2021-09-04T15:12:54.887702Z",
     "shell.execute_reply.started": "2021-09-04T15:12:54.851066Z"
    }
   },
   "outputs": [],
   "source": [
    "SPLIT_FRACTION = 0.9\n",
    "\n",
    "n_train_examples = int(len(raw_train_data) * SPLIT_FRACTION)\n",
    "n_valid_examples = len(raw_train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = random_split(dataset = raw_train_data,\n",
    "                                      lengths = [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the validation set uses the same transforms as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:12:59.470694Z",
     "iopub.status.busy": "2021-09-04T15:12:59.470381Z",
     "iopub.status.idle": "2021-09-04T15:12:59.594507Z",
     "shell.execute_reply": "2021-09-04T15:12:59.593386Z",
     "shell.execute_reply.started": "2021-09-04T15:12:59.470666Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_data = deepcopy(x = valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of examples in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:04.213449Z",
     "iopub.status.busy": "2021-09-04T15:13:04.213113Z",
     "iopub.status.idle": "2021-09-04T15:13:04.223048Z",
     "shell.execute_reply": "2021-09-04T15:13:04.221618Z",
     "shell.execute_reply.started": "2021-09-04T15:13:04.213419Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of examples in the training set: {len(train_data)}\")\n",
    "print(f\"Number of examples in the validation set: {len(valid_data)}\")\n",
    "print(f\"Number of examples in the test set: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot 25 sample images to check whether the proposed transforms are sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:08.175387Z",
     "iopub.status.busy": "2021-09-04T15:13:08.175015Z",
     "iopub.status.idle": "2021-09-04T15:13:08.181862Z",
     "shell.execute_reply": "2021-09-04T15:13:08.18033Z",
     "shell.execute_reply.started": "2021-09-04T15:13:08.175358Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    \n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min,\n",
    "                 max = image_max)\n",
    "    image.subtract_(image_min).div_(image_max - image_min + 1e-5)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:12.361877Z",
     "iopub.status.busy": "2021-09-04T15:13:12.36153Z",
     "iopub.status.idle": "2021-09-04T15:13:12.371562Z",
     "shell.execute_reply": "2021-09-04T15:13:12.369797Z",
     "shell.execute_reply.started": "2021-09-04T15:13:12.361833Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(images, \n",
    "                labels,\n",
    "                classes,\n",
    "                normalize = True):\n",
    "    \n",
    "    num_images = len(images)\n",
    "    \n",
    "    rows = int(np.sqrt(num_images))\n",
    "    columns = int(np.sqrt(num_images))\n",
    "    \n",
    "    fig = plt.figure(figsize = [10, 10])\n",
    "    \n",
    "    for i in range(rows * columns):\n",
    "        \n",
    "        ax = fig.add_subplot(rows, columns, (i+1))\n",
    "        \n",
    "        image = images[i]\n",
    "        \n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "            \n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(classes[labels[i]])\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:16.67383Z",
     "iopub.status.busy": "2021-09-04T15:13:16.673432Z",
     "iopub.status.idle": "2021-09-04T15:13:16.679018Z",
     "shell.execute_reply": "2021-09-04T15:13:16.677258Z",
     "shell.execute_reply.started": "2021-09-04T15:13:16.6738Z"
    }
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:19.978776Z",
     "iopub.status.busy": "2021-09-04T15:13:19.978407Z",
     "iopub.status.idle": "2021-09-04T15:13:21.748509Z",
     "shell.execute_reply": "2021-09-04T15:13:21.747429Z",
     "shell.execute_reply.started": "2021-09-04T15:13:19.978743Z"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = zip(*[(image, label) for (image, label) in \n",
    "                          [train_data[i] for i in range(N_IMAGES)]])\n",
    "\n",
    "classes = test_data.classes\n",
    "\n",
    "plot_images(images, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form data iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form iterators for the training, validation and test sets using a desired batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:34.052811Z",
     "iopub.status.busy": "2021-09-04T15:13:34.052429Z",
     "iopub.status.idle": "2021-09-04T15:13:34.05948Z",
     "shell.execute_reply": "2021-09-04T15:13:34.058121Z",
     "shell.execute_reply.started": "2021-09-04T15:13:34.05278Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:37.774141Z",
     "iopub.status.busy": "2021-09-04T15:13:37.77374Z",
     "iopub.status.idle": "2021-09-04T15:13:37.782311Z",
     "shell.execute_reply": "2021-09-04T15:13:37.778719Z",
     "shell.execute_reply.started": "2021-09-04T15:13:37.774111Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(dataset = train_data,\n",
    "                            shuffle = True,\n",
    "                            batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = DataLoader(dataset = valid_data,\n",
    "                            batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = DataLoader(dataset = test_data,\n",
    "                           batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a pretrained VGG16 model with batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:13:42.758363Z",
     "iopub.status.busy": "2021-09-04T15:13:42.757884Z",
     "iopub.status.idle": "2021-09-04T15:14:00.347178Z",
     "shell.execute_reply": "2021-09-04T15:14:00.346117Z",
     "shell.execute_reply.started": "2021-09-04T15:13:42.758316Z"
    }
   },
   "outputs": [],
   "source": [
    "model = vgg16_bn(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the layers present in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:05.235813Z",
     "iopub.status.busy": "2021-09-04T15:14:05.235472Z",
     "iopub.status.idle": "2021-09-04T15:14:05.243353Z",
     "shell.execute_reply": "2021-09-04T15:14:05.242106Z",
     "shell.execute_reply.started": "2021-09-04T15:14:05.235779Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the convolutional feature-extracting base is frozen and the classifier head is unfrozen. Setting requires_grad to False freezes the corresponding layer and setting it to True unfreezes it. The adaptive average pooling layer contains no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:10.481366Z",
     "iopub.status.busy": "2021-09-04T15:14:10.48099Z",
     "iopub.status.idle": "2021-09-04T15:14:10.488555Z",
     "shell.execute_reply": "2021-09-04T15:14:10.486611Z",
     "shell.execute_reply.started": "2021-09-04T15:14:10.481336Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:14.795918Z",
     "iopub.status.busy": "2021-09-04T15:14:14.795518Z",
     "iopub.status.idle": "2021-09-04T15:14:14.801902Z",
     "shell.execute_reply": "2021-09-04T15:14:14.800124Z",
     "shell.execute_reply.started": "2021-09-04T15:14:14.795886Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model was trained on the ImageNet dataset, which had 1000 classes. Hence, the final layer in the classifier has 1000 output features. Inspect this last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:19.959035Z",
     "iopub.status.busy": "2021-09-04T15:14:19.958628Z",
     "iopub.status.idle": "2021-09-04T15:14:19.96759Z",
     "shell.execute_reply": "2021-09-04T15:14:19.966179Z",
     "shell.execute_reply.started": "2021-09-04T15:14:19.959003Z"
    }
   },
   "outputs": [],
   "source": [
    "model.classifier[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am currently building a model on the CIFAR-10 dataset, which has 10 classes, I want this final layer to have 10 output features. Replace the final layer with a linear layer which has 10 output features. The newly added layer will have requires_grad set to True and will be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:29.150174Z",
     "iopub.status.busy": "2021-09-04T15:14:29.149744Z",
     "iopub.status.idle": "2021-09-04T15:14:29.157589Z",
     "shell.execute_reply": "2021-09-04T15:14:29.154284Z",
     "shell.execute_reply.started": "2021-09-04T15:14:29.150136Z"
    }
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "IN_FEATURES = model.classifier[-1].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:33.473786Z",
     "iopub.status.busy": "2021-09-04T15:14:33.473407Z",
     "iopub.status.idle": "2021-09-04T15:14:33.479324Z",
     "shell.execute_reply": "2021-09-04T15:14:33.478196Z",
     "shell.execute_reply.started": "2021-09-04T15:14:33.473739Z"
    }
   },
   "outputs": [],
   "source": [
    "final_layer = Linear(in_features = IN_FEATURES, \n",
    "                     out_features = N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:37.24804Z",
     "iopub.status.busy": "2021-09-04T15:14:37.247563Z",
     "iopub.status.idle": "2021-09-04T15:14:37.254022Z",
     "shell.execute_reply": "2021-09-04T15:14:37.252586Z",
     "shell.execute_reply.started": "2021-09-04T15:14:37.248008Z"
    }
   },
   "outputs": [],
   "source": [
    "model.classifier[-1] = final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the modified classifier does have 10 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:41.554409Z",
     "iopub.status.busy": "2021-09-04T15:14:41.554068Z",
     "iopub.status.idle": "2021-09-04T15:14:41.562385Z",
     "shell.execute_reply": "2021-09-04T15:14:41.560869Z",
     "shell.execute_reply.started": "2021-09-04T15:14:41.554379Z"
    }
   },
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the final overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:45.857916Z",
     "iopub.status.busy": "2021-09-04T15:14:45.85756Z",
     "iopub.status.idle": "2021-09-04T15:14:52.991735Z",
     "shell.execute_reply": "2021-09-04T15:14:52.990101Z",
     "shell.execute_reply.started": "2021-09-04T15:14:45.857887Z"
    }
   },
   "outputs": [],
   "source": [
    "summary(model,\n",
    "        input_size = [BATCH_SIZE, 3, 224, 224],\n",
    "        device = \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and the optimizer to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:14:59.553541Z",
     "iopub.status.busy": "2021-09-04T15:14:59.553108Z",
     "iopub.status.idle": "2021-09-04T15:14:59.561431Z",
     "shell.execute_reply": "2021-09-04T15:14:59.560026Z",
     "shell.execute_reply.started": "2021-09-04T15:14:59.553507Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(params = model.parameters(),\n",
    "                 lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:04.181483Z",
     "iopub.status.busy": "2021-09-04T15:15:04.181138Z",
     "iopub.status.idle": "2021-09-04T15:15:04.189356Z",
     "shell.execute_reply": "2021-09-04T15:15:04.188003Z",
     "shell.execute_reply.started": "2021-09-04T15:15:04.181453Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:08.302439Z",
     "iopub.status.busy": "2021-09-04T15:15:08.302054Z",
     "iopub.status.idle": "2021-09-04T15:15:08.308551Z",
     "shell.execute_reply": "2021-09-04T15:15:08.307135Z",
     "shell.execute_reply.started": "2021-09-04T15:15:08.302409Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:11.186127Z",
     "iopub.status.busy": "2021-09-04T15:15:11.185734Z",
     "iopub.status.idle": "2021-09-04T15:15:11.199475Z",
     "shell.execute_reply": "2021-09-04T15:15:11.197769Z",
     "shell.execute_reply.started": "2021-09-04T15:15:11.186095Z"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to train the model and simultaneously validate it, across a desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:17.85418Z",
     "iopub.status.busy": "2021-09-04T15:15:17.853811Z",
     "iopub.status.idle": "2021-09-04T15:15:17.870822Z",
     "shell.execute_reply": "2021-09-04T15:15:17.869448Z",
     "shell.execute_reply.started": "2021-09-04T15:15:17.85415Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iterator,\n",
    "          valid_iterator,\n",
    "          model,\n",
    "          loss_fn,\n",
    "          optimizer,\n",
    "          device,\n",
    "          n_epochs = 5):\n",
    "    \n",
    "    # Initialize the history list which will contain all losses and metrics\n",
    "    \n",
    "    history_list = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Start setting up the training procedure\n",
    "        \n",
    "        print(f\"Epoch {i+1}\")\n",
    "        print(\"-------------------------------\")\n",
    "    \n",
    "        train_size = len(train_iterator.dataset)\n",
    "        n_train_batches = len(train_iterator)\n",
    "        train_loss = 0\n",
    "        average_train_loss = 0\n",
    "        train_n_correct = 0\n",
    "        train_accuracy = 0\n",
    "    \n",
    "        # Set the model to training mode\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "        for train_batch, (X, y) in enumerate(train_iterator):\n",
    "        \n",
    "            # Copy the tensors to the GPU\n",
    "        \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Reset the gradients of the model parameters to zero\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Obtain the model prediction and loss\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Backpropagate the loss and deposit each gradient in place\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust the parameters using the gradients collected in the backward pass\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Increment the validation loss and the number of correctly labeled instances\n",
    "            # Build up these aggregate values instance by instance\n",
    "                \n",
    "            train_loss += loss.item()\n",
    "            train_n_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Display the training loss after every hundredth batch is trained\n",
    "        \n",
    "            if train_batch % 100 == 0:\n",
    "                loss = loss.item()\n",
    "                current_instance = train_batch * len(X)\n",
    "                print(f\"Loss: {loss:.6f} [{current_instance:5f} / {train_size:5f}]\")\n",
    "                \n",
    "        # Obtain average training loss and accuracy for the entire epoch\n",
    "        \n",
    "        average_train_loss = train_loss / n_train_batches\n",
    "        train_accuracy = train_n_correct / train_size\n",
    "                \n",
    "        # After training is finished, start validation\n",
    "        \n",
    "        valid_size = len(valid_iterator.dataset)\n",
    "        n_valid_batches = len(valid_iterator)\n",
    "        valid_loss = 0\n",
    "        average_valid_loss = 0\n",
    "        valid_n_correct = 0\n",
    "        valid_accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Set the model to evaluation mode\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for X, y in valid_iterator:\n",
    "                \n",
    "                # Copy the tensors to the GPU\n",
    "                \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # Obtain the model prediction and loss\n",
    "                \n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                # Increment the validation loss and the number of correctly labeled instances\n",
    "                # Build up aggregate values instance by instance\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "                valid_n_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                \n",
    "        # Obtain average validation loss and accuracy for the entire epoch\n",
    "        \n",
    "        average_valid_loss = valid_loss / n_valid_batches\n",
    "        valid_accuracy = valid_n_correct / valid_size\n",
    "        \n",
    "        print(\"Validation error:\")\n",
    "        print(f\"Accuracy: {valid_accuracy:.6f}, Average loss: {average_valid_loss:.6f}\")\n",
    "        print()\n",
    "        \n",
    "        history_list.append([average_train_loss, average_valid_loss, train_accuracy, valid_accuracy])\n",
    "    \n",
    "    # Display a message indicating training has finished\n",
    "    \n",
    "    print()\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Create a data frame containing the entire training and validation history\n",
    "    \n",
    "    history = pd.DataFrame(data = history_list,\n",
    "                           columns = [\"average_train_loss\", \"average_valid_loss\", \n",
    "                                      \"train_accuracy\", \"valid_accuracy\"])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a suitable number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:37.6064Z",
     "iopub.status.busy": "2021-09-04T15:15:37.605997Z",
     "iopub.status.idle": "2021-09-04T15:15:37.61189Z",
     "shell.execute_reply": "2021-09-04T15:15:37.609993Z",
     "shell.execute_reply.started": "2021-09-04T15:15:37.60637Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T15:15:42.027629Z",
     "iopub.status.busy": "2021-09-04T15:15:42.027256Z",
     "iopub.status.idle": "2021-09-04T17:20:26.520326Z",
     "shell.execute_reply": "2021-09-04T17:20:26.518101Z",
     "shell.execute_reply.started": "2021-09-04T15:15:42.027598Z"
    }
   },
   "outputs": [],
   "source": [
    "model, history = train(train_iterator = train_iterator,\n",
    "                       valid_iterator = valid_iterator,\n",
    "                       model = model,\n",
    "                       loss_fn = loss_fn,\n",
    "                       optimizer = optimizer,\n",
    "                       device = device,\n",
    "                       n_epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T17:20:38.323819Z",
     "iopub.status.busy": "2021-09-04T17:20:38.323435Z",
     "iopub.status.idle": "2021-09-04T17:20:38.569552Z",
     "shell.execute_reply": "2021-09-04T17:20:38.568467Z",
     "shell.execute_reply.started": "2021-09-04T17:20:38.323789Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 8])\n",
    "plt.plot(history[\"average_train_loss\"],\n",
    "         label = \"Average training loss\")\n",
    "plt.plot(history[\"average_valid_loss\"],\n",
    "         label = \"Average validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.title(\"Loss plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T17:20:48.635024Z",
     "iopub.status.busy": "2021-09-04T17:20:48.634573Z",
     "iopub.status.idle": "2021-09-04T17:20:48.868572Z",
     "shell.execute_reply": "2021-09-04T17:20:48.867299Z",
     "shell.execute_reply.started": "2021-09-04T17:20:48.634934Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 8])\n",
    "plt.plot(history[\"train_accuracy\"],\n",
    "         label = \"Training accuracy\")\n",
    "plt.plot(history[\"valid_accuracy\"],\n",
    "         label = \"Validation accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a final evaluation of the model on the test set. First, define a function to carry out the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T17:21:03.094869Z",
     "iopub.status.busy": "2021-09-04T17:21:03.094442Z",
     "iopub.status.idle": "2021-09-04T17:21:03.103419Z",
     "shell.execute_reply": "2021-09-04T17:21:03.102055Z",
     "shell.execute_reply.started": "2021-09-04T17:21:03.094837Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(test_iterator, \n",
    "             model, \n",
    "             loss_fn,\n",
    "             device):\n",
    "    \n",
    "    size = len(test_iterator.dataset)\n",
    "    num_batches = len(test_iterator)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iterator:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    average_loss = test_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    \n",
    "    print(\"Test error:\")\n",
    "    print(f\"Accuracy: {accuracy:.6f}, Average loss: {average_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the function on the model which has just been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T17:21:13.962376Z",
     "iopub.status.busy": "2021-09-04T17:21:13.962003Z",
     "iopub.status.idle": "2021-09-04T17:22:20.719311Z",
     "shell.execute_reply": "2021-09-04T17:22:20.717017Z",
     "shell.execute_reply.started": "2021-09-04T17:21:13.962345Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(test_iterator = test_iterator,\n",
    "         model = model,\n",
    "         loss_fn = loss_fn,\n",
    "         device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T17:22:35.586616Z",
     "iopub.status.busy": "2021-09-04T17:22:35.58622Z",
     "iopub.status.idle": "2021-09-04T17:22:37.927312Z",
     "shell.execute_reply": "2021-09-04T17:22:37.926197Z",
     "shell.execute_reply.started": "2021-09-04T17:22:35.58657Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"pytorch-vgg16-cifar-10-model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
